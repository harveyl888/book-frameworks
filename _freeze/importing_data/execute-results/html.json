{
  "hash": "4fb00bf96c46cea18c6bb2bdf78f7ef6",
  "result": {
    "engine": "knitr",
    "markdown": "# Importing Data {#sec-importing-data}\n\n## Introduction\n\nImporting data is a fundemental step for a data processing framework.  It's also quite simple due to the many functions in base R and R packages available for importing.  In this chapter we'll explore how an R-based framework can be used to import data.\n\n## A Simple Example\n\nWe will start with a simple example.  Our interpreter will simply read in data and print it to the console.  For the data we'll use the cricket dataset from TidyTuesday.\n\nOur json input file will look like this:\n\n```json\n{\n  \"dataset\": \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-11-30/matches.csv\"\n}\n```\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\nThis file contains just a single parameter, `dataset`, pointing to the URL of the dataset to read.  Our interpreter has a very simple purpose - read in the file and send the output to the console.  Our interpreter looks as follows:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninp <- jsonlite::fromJSON(\"data/myfile.json\", simplifyVector = FALSE)\ndata <- readr::read_csv(inp$dataset)\nprint(data)\n```\n:::\n\n\n\n\n\n\n\n\nThe output is:\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,237 × 24\n   match_id   team1    team2 score_team1 score_team2 wickets_team1 wickets_team2\n   <chr>      <chr>    <chr>       <dbl>       <dbl>         <dbl>         <dbl>\n 1 ODI # 1028 West In… Aust…         172         173             9             9\n 2 ODI # 1029 West In… Sri …         194         124            10            10\n 3 ODI # 1030 Sri Lan… West…         102         104            10             3\n 4 ODI # 1031 West In… Aust…         231         217            10            10\n 5 ODI # 1032 Austral… Sri …         213         214             5             7\n 6 ODI # 1033 South A… Engl…         211         205             8            10\n 7 ODI # 1034 South A… Engl…         262         265             8             5\n 8 ODI # 1035 Austral… Sri …         266         183             6             9\n 9 ODI # 1036 England  Sout…         198         199             8             7\n10 ODI # 1037 Sri Lan… West…         202         186            10             9\n# ℹ 1,227 more rows\n# ℹ 17 more variables: team1_away_or_home <chr>, team2_home_away <chr>,\n#   winner <chr>, margin <dbl>, margin_type <chr>, time_of_day <chr>,\n#   series <chr>, player_of_match <chr>, player_of_match_team <chr>,\n#   venue <chr>, toss <chr>, toss_decision <chr>, ball_remaining <chr>,\n#   ground <chr>, ground_city <chr>, ground_country <chr>, match_date <chr>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nWe have one parameter in our json file, `dataset`.  Changing it will change the dataset imported and printed.  Of course, if we had written an R script we could simply change the filename in the script itself or change a variable pointing to the script but using an external instruction file allows us to change the parameter without affecting the script itself.  For example, if we want chocolate ratings instead of cricket statistics we simply change the dataset parameter:\n\n```json\n{\n  \"dataset\": \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv\"\n}\n```\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\nUsing the same interpreter outputs the following:\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,530 × 10\n     ref company_manufacturer company_location review_date\n   <dbl> <chr>                <chr>                  <dbl>\n 1  2454 5150                 U.S.A.                  2019\n 2  2458 5150                 U.S.A.                  2019\n 3  2454 5150                 U.S.A.                  2019\n 4  2542 5150                 U.S.A.                  2021\n 5  2546 5150                 U.S.A.                  2021\n 6  2546 5150                 U.S.A.                  2021\n 7  2542 5150                 U.S.A.                  2021\n 8   797 A. Morin             France                  2012\n 9   797 A. Morin             France                  2012\n10  1011 A. Morin             France                  2013\n# ℹ 2,520 more rows\n# ℹ 6 more variables: country_of_bean_origin <chr>,\n#   specific_bean_origin_or_bar_name <chr>, cocoa_percent <chr>,\n#   ingredients <chr>, most_memorable_characteristics <chr>, rating <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nOur interpreter (which is very simple) can be used to read in and display either dataset.  Later chapters will explore how we can use a single interpreter to perform data manipulation and graphing for a number of different data imports.\n\n## Importing Different Data Formats\n\nBy introducing a conditional, an interpreter can import different data types.  \n\n### Introducing a parameter to specify the file type\n\nIn the most simple approach we can add a parameter to specify the dataset type and then respond accordingly.  For example adding a `format` parameter to our instruction file and responding to its value:\n\n```json\n{\n  \"format\": \"csv\",\n  \"dataset\": \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv\"\n}\n```\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninp <- jsonlite::fromJSON(\"data/myfile.json\", simplifyVector = FALSE)\nif (isTRUE(inp$format == \"csv\")) {\n  data <- readr::read_csv(inp$dataset)\n} else if (isTRUE(inp$format == \"sas\")) {\n  data <- haven::read_sas(inp$dataset)\n} else {\n  data <- NULL\n}\nprint(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,530 × 10\n     ref company_manufacturer company_location review_date\n   <dbl> <chr>                <chr>                  <dbl>\n 1  2454 5150                 U.S.A.                  2019\n 2  2458 5150                 U.S.A.                  2019\n 3  2454 5150                 U.S.A.                  2019\n 4  2542 5150                 U.S.A.                  2021\n 5  2546 5150                 U.S.A.                  2021\n 6  2546 5150                 U.S.A.                  2021\n 7  2542 5150                 U.S.A.                  2021\n 8   797 A. Morin             France                  2012\n 9   797 A. Morin             France                  2012\n10  1011 A. Morin             France                  2013\n# ℹ 2,520 more rows\n# ℹ 6 more variables: country_of_bean_origin <chr>,\n#   specific_bean_origin_or_bar_name <chr>, cocoa_percent <chr>,\n#   ingredients <chr>, most_memorable_characteristics <chr>, rating <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nWe have introduced a conditional that responds to the `format` parameter using {readr} to import the data if `format` is *csv* and {haven} to import the data if `format` is *sas*.  We also include a step to catch the fall-through if `format` is neither *csv* or *sas*.  \nIt is worth noting that `isTRUE` is used around the conditions.  This is because the input file may contain a record in which `format` has not been set.  In this case, once the json file is imported and converted to an R list, `inp$format` would be `NULL`.  and `inp$format == \"csv\"` equates to logical(0).  Wrapping the condition in `isTRUE` will still return a logical if part of the logical is NULL so if `inp$format` is `NULL`, `isTRUE(inp$format == \"csv\")` returns `FALSE`.\n\n::: {.callout-important}\n## A note on using dplyr::case_when()\nAn alternative, and more succinct approach, would be to use `case_when()` from {dplyr}:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninp <- jsonlite::fromJSON(\"data/myfile.json\", simplifyVector = FALSE)\ndata <- dplyr::case_when(\n  isTRUE(inp$format == \"csv\") ~ readr::read_csv(inp$dataset),\n  isTRUE(inp$format == \"sas\") ~ haven::read_sas(inp$dataset),\n  TRUE ~ NULL\n)\nprint(data)\n```\n:::\n\n\n\n\n\n\n\n\nUnfortunately, the code above results in an error as `case_when()` evaluates all right-hand side expressions prior to returning the output.  If the file is a CSV file, as is the case in this example, the expression `haven::read_sas(inp$dataset)` leads to an error and the `case_when()` expression fails.\n:::\n\n### Deducing the file type from the file extension\n\nSince the file format can generally be deduced from the file extension, we could even drop the dependence on the `format` parameter and build the condition on the file extension:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninp <- jsonlite::fromJSON(\"data/myfile.json\", simplifyVector = FALSE)\nif (isTRUE(tools::file_ext(inp$dataset) == \"csv\")) {\n  data <- readr::read_csv(inp$dataset)\n} else if (isTRUE(tools::file_ext(inp$dataset) == \"sas\")) {\n  data <- haven::read_sas(inp$dataset)\n} else {\n  data <- NULL\n}\nprint(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,530 × 10\n     ref company_manufacturer company_location review_date\n   <dbl> <chr>                <chr>                  <dbl>\n 1  2454 5150                 U.S.A.                  2019\n 2  2458 5150                 U.S.A.                  2019\n 3  2454 5150                 U.S.A.                  2019\n 4  2542 5150                 U.S.A.                  2021\n 5  2546 5150                 U.S.A.                  2021\n 6  2546 5150                 U.S.A.                  2021\n 7  2542 5150                 U.S.A.                  2021\n 8   797 A. Morin             France                  2012\n 9   797 A. Morin             France                  2012\n10  1011 A. Morin             France                  2013\n# ℹ 2,520 more rows\n# ℹ 6 more variables: country_of_bean_origin <chr>,\n#   specific_bean_origin_or_bar_name <chr>, cocoa_percent <chr>,\n#   ingredients <chr>, most_memorable_characteristics <chr>, rating <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThus, our instruction file could revert back to the shorter version in which the file type is not specified:\n\n```json\n{\n  \"dataset\": \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv\"\n}\n```\n\n## When to Import Data\n\n### Import Once or Many?'\n\nData may be imported upon startup or imported as needed.  Depending on the type and size of data, data import may be a slow step and it is prudent to consider the type and size of data required by a shiny framework at the design stage.  Let's consider the two options of importing all data at the start or only importing when data are required.\n\n### Import data once at the start\n\nIt makes sense to import data just once, when the app starts up, if the framework relies on only a few data sources.  This occurs when the framework defines a single instruction set or if we have a collection of instructions but these instructions only access a limited number of datasets.\n\n### Import data as needed\n\nImporting data as needed makes sense when you have a collection of instructions that access a large number of datasets - for example if each instruction set accesses a different dataset.  This is particularly true if instruction sets are selected by a user.  For example, consider an app with 200 instructions and outputs where the user may choose which one to process and display.  In such a case we should not import all 200 datasets upfront as only one is required by the app.  The dataset should be imported once the user has selected the instruction set.\n\n## Faster Data Import\n\nData may come in many formats.  This section is not designed to describe all the different formats and ways to import them, but rather to highlight three methods to improve data import.  We'll consider CSV formatted data since it is a common and popular format.\n\n### Choice of R Package\n\nThe base R CSV reader function `read.csv()` is relatively slow.  There are faster CSV file readers that may be used in its place such as `read_csv()` from {readr} and `fread` from {data.table}.\n\nTo test speed we'll create two CSV files.  `f1.csv` contains 10000 rows of 10 columns of data (first three columns are character and rest are numeric).  `f2.csv` contains 10 rows of 10000 columns of data (first three columns are character and rest are numeric).  `f1.csv` simulates long data whereas `f2.csv` simulates wide.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create two CSV files\n## f1.csv - 10000 rows, 10 cols, long format\n## f2.csv - 10 rows, 10000 cols, wide format\nd <- list(c(10000, 10), c(10, 10000))\nfor (i in seq_along(d)) {\n  f <- sprintf(\"data/f_%02i.csv\", i)\n  data <- matrix(runif(d[[i]][1] * d[[i]][2]), ncol = d[[i]][2]) |>\n    data.frame() |>\n    dplyr::mutate(dplyr::across(c(\"X1\", \"X2\", \"X3\"), as.character))\n  write.csv(data, f, row.names = FALSE)\n}\n```\n:::\n\n\n\n\n\n\n\n\nWe can test the speed of `base::read.csv`, `readr::read_csv` and `data.table::fread` using {microbenchmark} and visualize the output with `ggplot::autoplot` as follows\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_read_all <- lapply(seq(2), function(i) {\n  f <- sprintf(\"data/f_%02i.csv\", i)\n  bench::mark(\n    `read.csv` = read.csv(f),\n    read_csv = readr::read_csv(f, show_col_types = FALSE),\n    fread = data.table::fread(f), \n    iterations = 10, check = FALSE\n  )\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tidyr::unnest(out_read_all[[1]], c(time, gc)) |> \n  dplyr::mutate(expression = factor(expression, levels = c(\"fread\", \"read_csv\", \"read.csv\")))\nggplot2::ggplot(df, ggplot2::aes(expression, time)) + \n  ggplot2::geom_violin() + \n  ggplot2::coord_flip() +\n  ggplot2::ggtitle(\"Time to read long-format file\")\n```\n\n::: {.cell-output-display}\n![](importing_data_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tidyr::unnest(out_read_all[[2]], c(time, gc)) |> \n  dplyr::mutate(expression = factor(expression, levels = c(\"fread\", \"read_csv\", \"read.csv\")))\nggplot2::ggplot(df, ggplot2::aes(expression, time)) + \n  ggplot2::geom_violin() + \n  ggplot2::coord_flip() +\n  ggplot2::ggtitle(\"Time to read wide-format file\")\n```\n\n::: {.cell-output-display}\n![](importing_data_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n:::{.callout-note}\nLong format relative timing: fread: 1, read_csv: 13.2, read.csv: 12.8  \nWide format relative timing: fread: 1, read_csv: 967, read.csv: 703  \n\n-  `data.table::fread` is the most efficient at reading both long and wide formatted CSV data.\n-  reading wide-formatted files is much slower than long-formatted files\n:::\n\n### Importing Specific Columns\n\nBoth `readr::read_csv` and `data.table::fread` include a parameter to limit the columns imported.  If you know that not all columns are required, limiting the imported columns can speed up the time.  In the example below we read in just 6 columns.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_select <- lapply(seq(2), function(i) {\n  f <- sprintf(\"data/f_%02i.csv\", i)\n  chosen_cols <- c(\"X1\", paste0(\"X\", sample(x = seq(d[[i]][2])[-1], size = 5)))\n  bench::mark(\n    `read.csv` = read.csv(f),\n    read_csv = readr::read_csv(f, show_col_types = FALSE, col_select = all_of(chosen_cols)),\n    fread = data.table::fread(f, select = chosen_cols),\n    iterations = 10, check = FALSE\n  )\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tidyr::unnest(out_select[[1]], c(time, gc)) |> \n  dplyr::mutate(expression = factor(expression, levels = c(\"fread\", \"read_csv\", \"read.csv\")))\nggplot2::ggplot(df, ggplot2::aes(expression, time)) + \n  ggplot2::geom_violin() + \n  ggplot2::coord_flip() +\n  ggplot2::ggtitle(\"Time to read long-format file (selected columns)\")\n```\n\n::: {.cell-output-display}\n![](importing_data_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tidyr::unnest(out_select[[2]], c(time, gc)) |> \n  dplyr::mutate(expression = factor(expression, levels = c(\"fread\", \"read_csv\", \"read.csv\")))\nggplot2::ggplot(df, ggplot2::aes(expression, time)) + \n  ggplot2::geom_violin() + \n  ggplot2::coord_flip() +\n  ggplot2::ggtitle(\"Time to read wide-format file (selected columns)\")\n```\n\n::: {.cell-output-display}\n![](importing_data_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n:::{.callout-note}\nLong format relative timing: fread: 1, read_csv: 8.73, read.csv: 14.9  \nWide format relative timing: fread: 1, read_csv: 3.84, read.csv: 761  \n\n-  data.table::fread is the most efficient at reading both long and wide formatted CSV data.\n-  There is little advantage when selecting columns for `data.table::fread` but when selected with `readr::read_csv` there is a significant increase in speed (1.58x for long data and 278x for wide data)\n:::\n\nThis knowledge can be carried over to our interpreter.  For example, the interpreter code could be updated as follows so that only necessary columns are imported:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninp <- jsonlite::fromJSON(\"data/myfile.json\", simplifyVector = FALSE)\nonly_cols <- unlist(inp$columns)\nif (isTRUE(tools::file_ext(inp$dataset) == \"csv\")) {\n  data <- readr::read_csv(inp$dataset, col_select = only_cols)\n} else if (isTRUE(tools::file_ext(inp$dataset) == \"sas\")) {\n  data <- haven::read_sas(inp$dataset, col_select = only_cols)\n} else {\n  data <- NULL\n}\nprint(data)\n```\n:::\n\n\n\n\n\n\n\n\nand our instruction file should be updated as follows:\n\n```json\n{\n  \"dataset\": \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv\",\n  \"columns\": [\"ref\", \"cocoa_percent\", \"rating\"]\n}\n```\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\nThis evaluates to:\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,530 × 3\n     ref cocoa_percent rating\n   <dbl> <chr>          <dbl>\n 1  2454 76%             3.25\n 2  2458 76%             3.5 \n 3  2454 76%             3.75\n 4  2542 68%             3   \n 5  2546 72%             3   \n 6  2546 80%             3.25\n 7  2542 68%             3.5 \n 8   797 70%             3.5 \n 9   797 63%             3.75\n10  1011 70%             2.75\n# ℹ 2,520 more rows\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}