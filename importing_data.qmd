# Importing Data {#sec-importing-data}

## Introduction

Importing data is a fundemental step for a data processing framework.  It's also quite simple due to the many functions in base R and R packages available for importing.  In this chapter we'll explore how an R-based framework can be used to import data.

## A Simple Example

We will start with a simple example.  Our interpreter will simply read in data and print it to the console.  For the data we'll use the cricket dataset from TidyTuesday.

Our json input file will look like this:

```json
{
  "dataset": "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-11-30/matches.csv"
}
```

```{r}
#| echo: false
#| eval: true
jsonlite::write_json(list(dataset="https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-11-30/matches.csv"), 'data/myfile.json', auto_unbox = TRUE)
```

This file contains just a single parameter, `dataset`, pointing to the URL of the dataset to read.  Our interpreter has a very simple purpose - read in the file and send the output to the console.  Our interpreter looks as follows:

```{r}
#| message: false
#| eval: false
inp <- jsonlite::fromJSON("data/myfile.json", simplifyVector = FALSE)
data <- readr::read_csv(inp$dataset)
print(data)
```

The output is:
```{r}
#| message: false
#| eval: true
#| echo: False
inp <- jsonlite::fromJSON("data/myfile.json", simplifyVector = FALSE)
data <- readr::read_csv(inp$dataset)
print(data)
```

We have one parameter in our json file, `dataset`.  Changing it will change the dataset imported and printed.  Of course, if we had written an R script we could simply change the filename in the script itself or change a variable pointing to the script but using an external instruction file allows us to change the parameter without affecting the script itself.  For example, if we want chocolate ratings instead of cricket statistics we simply change the dataset parameter:

```json
{
  "dataset": "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv"
}
```

```{r}
#| echo: false
#| eval: true
jsonlite::write_json(list(dataset="https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv"), 'data/myfile.json', auto_unbox = TRUE)
```

Using the same interpreter outputs the following:

```{r}
#| eval: true
#| echo: False
#| message: false
inp <- jsonlite::fromJSON("data/myfile.json", simplifyVector = FALSE)
data <- readr::read_csv(inp$dataset)
print(data)
```

Our interpreter (which is very simple) can be used to read in and display either dataset.  Later chapters will explore how we can use a single interpreter to perform data manipulation and graphing for a number of different data imports.

## Importing Different Data Formats

By introducing a conditional, an interpreter can import different data types.  

### Introducing a parameter to specify the file type

In the most simple approach we can add a parameter to specify the dataset type and then respond accordingly.  For example adding a `format` parameter to our instruction file and responding to its value:

```json
{
  "format": "csv",
  "dataset": "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv"
}
```

```{r}
#| echo: false
#| eval: true
jsonlite::write_json(list(format="csv", dataset="https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv"), 'data/myfile.json', auto_unbox = TRUE)
```

```{r}
#| message: false
inp <- jsonlite::fromJSON("data/myfile.json", simplifyVector = FALSE)
if (isTRUE(inp$format == "csv")) {
  data <- readr::read_csv(inp$dataset)
} else if (isTRUE(inp$format == "sas")) {
  data <- haven::read_sas(inp$dataset)
} else {
  data <- NULL
}
print(data)
```

We have introduced a conditional that responds to the `format` parameter using {readr} to import the data if `format` is *csv* and {haven} to import the data if `format` is *sas*.  We also include a step to catch the fall-through if `format` is neither *csv* or *sas*.  
It is worth noting that `isTRUE` is used around the conditions.  This is because the input file may contain a record in which `format` has not been set.  In this case, once the json file is imported and converted to an R list, `inp$format` would be `NULL`.  and `inp$format == "csv"` equates to logical(0).  Wrapping the condition in `isTRUE` will still return a logical if part of the logical is NULL so if `inp$format` is `NULL`, `isTRUE(inp$format == "csv")` returns `FALSE`.

::: {.callout-important}
## A note on using dplyr::case_when()
An alternative, and more succinct approach, would be to use `case_when()` from {dplyr}:

```{r}
#| eval: false
inp <- jsonlite::fromJSON("data/myfile.json", simplifyVector = FALSE)
data <- dplyr::case_when(
  isTRUE(inp$format == "csv") ~ readr::read_csv(inp$dataset),
  isTRUE(inp$format == "sas") ~ haven::read_sas(inp$dataset),
  TRUE ~ NULL
)
print(data)
```

Unfortunately, the code above results in an error as `case_when()` evaluates all right-hand side expressions prior to returning the output.  If the file is a CSV file, as is the case in this example, the expression `haven::read_sas(inp$dataset)` leads to an error and the `case_when()` expression fails.
:::

### Deducing the file type from the file extension

Since the file format can generally be deduced from the file extension, we could even drop the dependence on the `format` parameter and build the condition on the file extension:

```{r}
#| echo: false
#| eval: true
jsonlite::write_json(list(dataset="https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv"), 'data/myfile.json', auto_unbox = TRUE)
```

```{r}
#| message: false
inp <- jsonlite::fromJSON("data/myfile.json", simplifyVector = FALSE)
if (isTRUE(tools::file_ext(inp$dataset) == "csv")) {
  data <- readr::read_csv(inp$dataset)
} else if (isTRUE(tools::file_ext(inp$dataset) == "sas")) {
  data <- haven::read_sas(inp$dataset)
} else {
  data <- NULL
}
print(data)
```

Thus, our instruction file could revert back to the shorter version in which the file type is not specified:

```json
{
  "dataset": "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv"
}
```

## When to Import Data

### Import Once or Many?'

Data may be imported upon startup or imported as needed.  Depending on the type and size of data, data import may be a slow step and it is prudent to consider the type and size of data required by a shiny framework at the design stage.  Let's consider the two options of importing all data at the start or only importing when data are required.

### Import data once at the start

It makes sense to import data just once, when the app starts up, if the framework relies on only a few data sources.  This occurs when the framework defines a single instruction set or if we have a collection of instructions but these instructions only access a limited number of datasets.

### Import data as needed

Importing data as needed makes sense when you have a collection of instructions that access a large number of datasets - for example if each instruction set accesses a different dataset.  This is particularly true if instruction sets are selected by a user.  For example, consider an app with 200 instructions and outputs where the user may choose which one to process and display.  In such a case we should not import all 200 datasets upfront as only one is required by the app.  The dataset should be imported once the user has selected the instruction set.

## Faster Data Import

Data may come in many formats.  This section is not designed to describe all the different formats and ways to import them, but rather to highlight three methods to improve data import.  We'll consider CSV formatted data since it is a common and popular format.

### Choice of R Package

The base R CSV reader function `read.csv()` is relatively slow.  There are faster CSV file readers that may be used in its place such as `read_csv()` from {readr} and `fread` from {data.table}.

To test speed we'll create two CSV files.  `f1.csv` contains 10000 rows of 10 columns of data (first three columns are character and rest are numeric).  `f2.csv` contains 10 rows of 10000 columns of data (first three columns are character and rest are numeric).  `f1.csv` simulates long data whereas `f2.csv` simulates wide.

```{r}
#| cache: true
## create two CSV files
## f1.csv - 10000 rows, 10 cols, long format
## f2.csv - 10 rows, 10000 cols, wide format
d <- list(c(10000, 10), c(10, 10000))
for (i in seq_along(d)) {
  f <- sprintf("data/f_%02i.csv", i)
  data <- matrix(runif(d[[i]][1] * d[[i]][2]), ncol = d[[i]][2]) |>
    data.frame() |>
    dplyr::mutate(dplyr::across(c("X1", "X2", "X3"), as.character))
  write.csv(data, f, row.names = FALSE)
}
```

We can test the speed of `base::read.csv`, `readr::read_csv` and `data.table::fread` using {microbenchmark} and visualize the output with `ggplot::autoplot` as follows

```{r}
#| cache: true
#| warning: false
out_read_all <- lapply(seq(2), function(i) {
  f <- sprintf("data/f_%02i.csv", i)
  bench::mark(
    `read.csv` = read.csv(f),
    read_csv = readr::read_csv(f, show_col_types = FALSE),
    fread = data.table::fread(f), 
    iterations = 10, check = FALSE
  )
})
```

```{r}
#| cache: true
#| message: false
df <- tidyr::unnest(out_read_all[[1]], c(time, gc)) |> 
  dplyr::mutate(expression = factor(expression, levels = c("fread", "read_csv", "read.csv")))
ggplot2::ggplot(df, ggplot2::aes(expression, time)) + 
  ggplot2::geom_violin() + 
  ggplot2::coord_flip() +
  ggplot2::ggtitle("Time to read long-format file")
```

```{r}
#| cache: true
#| message: false
df <- tidyr::unnest(out_read_all[[2]], c(time, gc)) |> 
  dplyr::mutate(expression = factor(expression, levels = c("fread", "read_csv", "read.csv")))
ggplot2::ggplot(df, ggplot2::aes(expression, time)) + 
  ggplot2::geom_violin() + 
  ggplot2::coord_flip() +
  ggplot2::ggtitle("Time to read wide-format file")
```

:::{.callout-note}
Long format relative timing: fread: 1, read_csv: `r signif(as.numeric(out_read_all[[1]]$min[2])/as.numeric(out_read_all[[1]]$min[3]), 3)`, read.csv: `r signif(as.numeric(out_read_all[[1]]$min[1])/as.numeric(out_read_all[[1]]$min[3]), 3)`  
Wide format relative timing: fread: 1, read_csv: `r signif(as.numeric(out_read_all[[2]]$min[2])/as.numeric(out_read_all[[2]]$min[3]), 3)`, read.csv: `r signif(as.numeric(out_read_all[[2]]$min[1])/as.numeric(out_read_all[[2]]$min[3]), 3)`  

-  `data.table::fread` is the most efficient at reading both long and wide formatted CSV data.
-  reading wide-formatted files is much slower than long-formatted files
:::

### Importing Specific Columns

Both `readr::read_csv` and `data.table::fread` include a parameter to limit the columns imported.  If you know that not all columns are required, limiting the imported columns can speed up the time.  In the example below we read in just 6 columns.

```{r}
#| cache: true
#| warning: false
out_select <- lapply(seq(2), function(i) {
  f <- sprintf("data/f_%02i.csv", i)
  chosen_cols <- c("X1", paste0("X", sample(x = seq(d[[i]][2])[-1], size = 5)))
  bench::mark(
    `read.csv` = read.csv(f),
    read_csv = readr::read_csv(f, show_col_types = FALSE, col_select = all_of(chosen_cols)),
    fread = data.table::fread(f, select = chosen_cols),
    iterations = 10, check = FALSE
  )
})
```


```{r}
#| cache: true
#| message: false
df <- tidyr::unnest(out_select[[1]], c(time, gc)) |> 
  dplyr::mutate(expression = factor(expression, levels = c("fread", "read_csv", "read.csv")))
ggplot2::ggplot(df, ggplot2::aes(expression, time)) + 
  ggplot2::geom_violin() + 
  ggplot2::coord_flip() +
  ggplot2::ggtitle("Time to read long-format file (selected columns)")
```

```{r}
#| cache: true
#| message: false
df <- tidyr::unnest(out_select[[2]], c(time, gc)) |> 
  dplyr::mutate(expression = factor(expression, levels = c("fread", "read_csv", "read.csv")))
ggplot2::ggplot(df, ggplot2::aes(expression, time)) + 
  ggplot2::geom_violin() + 
  ggplot2::coord_flip() +
  ggplot2::ggtitle("Time to read wide-format file (selected columns)")
```

:::{.callout-note}
Long format relative timing: fread: 1, read_csv: `r signif(as.numeric(out_select[[1]]$min[2])/as.numeric(out_select[[1]]$min[3]), 3)`, read.csv: `r signif(as.numeric(out_select[[1]]$min[1])/as.numeric(out_select[[1]]$min[3]), 3)`  
Wide format relative timing: fread: 1, read_csv: `r signif(as.numeric(out_select[[2]]$min[2])/as.numeric(out_select[[2]]$min[3]), 3)`, read.csv: `r signif(as.numeric(out_select[[2]]$min[1])/as.numeric(out_select[[2]]$min[3]), 3)`  

-  data.table::fread is the most efficient at reading both long and wide formatted CSV data.
-  There is little advantage when selecting columns for `data.table::fread` but when selected with `readr::read_csv` there is a significant increase in speed (`r signif(as.numeric(out_read_all[[1]]$min[2])/as.numeric(out_select[[1]]$min[2]), 3)`x for long data and `r signif(as.numeric(out_read_all[[2]]$min[2])/as.numeric(out_select[[2]]$min[2]), 3)`x for wide data)
:::

This knowledge can be carried over to our interpreter.  For example, the interpreter code could be updated as follows so that only necessary columns are imported:

```{r}
#| message: false
#| eval: false
inp <- jsonlite::fromJSON("data/myfile.json", simplifyVector = FALSE)
only_cols <- unlist(inp$columns)
if (isTRUE(tools::file_ext(inp$dataset) == "csv")) {
  data <- readr::read_csv(inp$dataset, col_select = only_cols)
} else if (isTRUE(tools::file_ext(inp$dataset) == "sas")) {
  data <- haven::read_sas(inp$dataset, col_select = only_cols)
} else {
  data <- NULL
}
print(data)
```

and our instruction file should be updated as follows:

```json
{
  "dataset": "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv",
  "columns": ["ref", "cocoa_percent", "rating"]
}
```

```{r}
#| echo: false
#| eval: true
jsonlite::write_json(list(dataset="https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv", columns=list("ref", "cocoa_percent", "rating")), 'data/myfile.json', auto_unbox = TRUE)
```

This evaluates to:

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
inp <- jsonlite::fromJSON("data/myfile.json", simplifyVector = FALSE)
only_cols <- unlist(inp$columns)
if (isTRUE(tools::file_ext(inp$dataset) == "csv")) {
  data <- readr::read_csv(inp$dataset, col_select = only_cols)
} else if (isTRUE(tools::file_ext(inp$dataset) == "sas")) {
  data <- haven::read_sas(inp$dataset, col_select = only_cols)
} else {
  data <- NULL
}
print(data)
```
